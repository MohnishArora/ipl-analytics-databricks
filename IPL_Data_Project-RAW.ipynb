{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da743590-1826-4d64-b558-07113d40605b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Data Analytics of IPL match data - 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37bb38d-fab8-4a77-b625-82d2e206463d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will focus more on data engineering part involved in this project, focsuing on the use of S3 buckets, spark, SQL and ML to create an environment for analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47587e7a-0a04-4c14-a2c9-2375450f3a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Video referred for this - https://www.youtube.com/watch?v=0iNJPKheQqM&list=PLBJe2dFI4sgvQTNNkI3ETYJgNPR4CBpFd_\n",
    "\n",
    "_Data Set Referred from - https://data.world/raghu543/ipl-data-till-2017_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98e259a-9129-41c4-abbc-80de6c49b082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Before we get down to the ingestion part, there is a most important step in connecting the DBs session with AWS. Below I am highlighting the steps which I found on research.**\n",
    "\n",
    "###Phase 1: AWS IAM role setup\n",
    "This phase focused on creating an IAM role in AWS that Databricks Unity Catalog could assume to access your S3 data securely.\n",
    "1. Create an IAM Role with a Temporary Trust Policy: Create a new IAM role in the AWS Console with a custom trust policy. This initial policy included the Databricks Unity Catalog service ARN and a placeholder External ID (\"0000\") because the role's own ARN was not available yet. The trust policy grants the sts:AssumeRole action under specific conditions. \n",
    "\n",
    "2. Attach a Permissions Policy for S3: Attach an S3 permissions policy to the IAM role during creation (I attached complete S3 access policy). This policy grants necessary permissions to your S3 bucket, such as s3:ListBucket and s3:GetObject. \n",
    "\n",
    "3. Name and Create the IAM Role: Name the role descriptively and complete its creation. \n",
    "\n",
    "###Phase 2: Databricks credential and external location setup\n",
    "This phase established the connection between Databricks and the created IAM role.\n",
    "4. Create a Storage Credential in Databricks: In the Databricks Catalog Explorer, create a storage credential (Manual), entering the ARN of the IAM role you created in AWS. Databricks then provided a unique External ID, which I copied.\n",
    "\n",
    "5. Finalize the IAM Role's Trust Policy in AWS: Return to the AWS Console to edit the trust policy of the IAM role. Update policy to include both the Databricks service ARN and the role's own ARN in the Principal section. Then replace the temporary \"0000\" placeholder with the External ID obtained from Databricks.\n",
    "\n",
    "6. Validate the Storage Credential in Databricks: Back in Databricks, validate the storage credential to confirm that Databricks could successfully assume the configured role. This is done by heading to _Catalog in Databricks >> Manage >> Credentials >> click on the storage cred created >> Validate Configuration_\n",
    "\n",
    "7. Create an External Location in Databricks: Create an external location in Databricks' Catalog Explorer, specifying your S3 path (e.g., s3://your-bucket-name/) and associating it with the validated storage credential. We can test this connection to verify it is working well or not.\n",
    "\n",
    "###Phase 3: Ingesting data\n",
    "8. Run Spark Code to Ingest Data: With the setup complete, you were able to execute a Spark command within a Databricks notebook to read the CSV data from your designated S3 path. An example Spark code snippet is provided in the referenced document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdeeadf4-2fc6-4317-957b-56750cc20a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257bfb3a-d19e-477d-82b7-954cbe42a4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Create a spark session to host the ingestion and data transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994ca49d-1603-435d-b1c2-79ea789dbfd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"IPL Data Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c970fe-f744-425b-b671-fb3e597c5c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import data from S3 bucket using spark session created\n",
    "\n",
    "Ball_By_Ball_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"s3://ipl-data-analysis-project-deproj/Ball_By_Ball.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05e27e8-8360-444d-9e0e-2d1944f052c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The above schema shows that some data types of the columns are not imported properly. This is common challenge with Spark as it is difficult for it to always infer the exact format from the external file sources to be imported.\n",
    "\n",
    "- This is why creating your own schemas while ingesting data is important, since this involves steps that make sure the data being updated/imported to a specific column/field inherets that fields data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034ab7ce-9894-434a-95af-1e7d17bc2f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Enter StructField and StructType - To create a structured schema for a spark table\n",
    "\n",
    "- We will use the pyspark.sql.types library to define a fixed schema for the target tables where the data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9057ceaf-d493-4d37-97e1-61b6fcf3fd21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType, DecimalType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bf540a-280f-4f1d-99c2-0568f651314e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Once we run the above code we will then define the structure schema.\n",
    "- We will be defining a struct schema for ball_by_ball data which we imported from S3 bucket. \n",
    "  - Define a dataframe for the schema : ball_by_ball_schema\n",
    "  - Use the following code for each data type you want to use for each column:\n",
    "      'StringType([StructField(<column_name>), IntegerType(), True <if you want to allow nulls>])'\n",
    "\n",
    "- There are many colummns (in ball_by_ball case, 48) and to write a code for each of them will be a task. This is where AI comes into play. \n",
    "- Copy the table schema from the online data set data dictionary and paste it in any AI text engine. Prompt the AI engine to create a spark code for creating a structure schema for a table whose data dictionay is shared. Below is my prompt which I will be using for ChatGPT to generate a struct schema for my table: \n",
    "\n",
    "  \" I am sharing my dataframe in this prompt. I want you to generate PySpark Structure (struct) schema using the schema of my dataframe shared below. \"\n",
    "\n",
    "- Below is the code generated by ChatGPT based on my above prompt and the data dictionary I shared with it. In this way we can reduce the manual task and focus on getting the task done quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af37280c-0e8f-41ec-954b-ac47678d1659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, BooleanType, DateType\n",
    ")'''\n",
    "\n",
    "ball_by_ball_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"over_id\", IntegerType(), True),\n",
    "    StructField(\"ball_id\", IntegerType(), True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", BooleanType(), True),\n",
    "    StructField(\"bowled\", BooleanType(), True),\n",
    "    StructField(\"run_out\", BooleanType(), True),\n",
    "    StructField(\"lbw\", BooleanType(), True),\n",
    "    StructField(\"retired_hurt\", BooleanType(), True),\n",
    "    StructField(\"stumped\", BooleanType(), True),\n",
    "    StructField(\"caught_and_bowled\", BooleanType(), True),\n",
    "    StructField(\"hit_wicket\", BooleanType(), True),\n",
    "    StructField(\"obstructingfeild\", BooleanType(), True),\n",
    "    StructField(\"bowler_wicket\", BooleanType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", BooleanType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", DateType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ee1514-9f10-454d-82ed-4d8ec18810b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Combining all the above steps to get the correct schema type of data saved into our imported table\n",
    "\n",
    "1. Use the schema code created by AI to create our schema, run it.\n",
    "\n",
    "2. Change the df code to import data from S3 bucket such that we are importing the S3 data into the schema created.\n",
    "\n",
    "3. Import the schema data from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb6ca2a-4c1f-4c82-8c71-8775feb9b388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#0. Creating a spark session for the applicaiton to run\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"IPL Data Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9b64b5-505d-4db2-a8c7-41198ec95d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Use the schema code created by AI to create our schema, run it.\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, BooleanType, DateType, DecimalType\n",
    ")\n",
    "\n",
    "ball_by_ball_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"over_id\", IntegerType(), True),\n",
    "    StructField(\"ball_id\", IntegerType(), True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", BooleanType(), True),\n",
    "    StructField(\"bowled\", BooleanType(), True),\n",
    "    StructField(\"run_out\", BooleanType(), True),\n",
    "    StructField(\"lbw\", BooleanType(), True),\n",
    "    StructField(\"retired_hurt\", BooleanType(), True),\n",
    "    StructField(\"stumped\", BooleanType(), True),\n",
    "    StructField(\"caught_and_bowled\", BooleanType(), True),\n",
    "    StructField(\"hit_wicket\", BooleanType(), True),\n",
    "    StructField(\"obstructingfeild\", BooleanType(), True),\n",
    "    StructField(\"bowler_wicket\", BooleanType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", BooleanType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", DateType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a47f2d-7767-49b7-945a-991ee6957e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Change the df code to import data from S3 bucket such that we are importing the S3 data into the schema created.\n",
    "\n",
    "'''Ball_By_Ball_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"s3://ipl-data-analysis-project-deproj/Ball_By_Ball.csv\")''' #Initial schema to import external data as it is\n",
    "\n",
    "#Changed code below\n",
    "Ball_By_Ball_df = spark.read.schema(ball_by_ball_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project-deproj/Ball_By_Ball.csv\")\n",
    "\n",
    "#3. Run this code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fce51eb-4a6b-4a84-a5fb-604ac664e797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If you compare the dataframe of the above code with the one we created without using schema, you can see Spark has retained the data type we defined for the our schema table here. \n",
    "\n",
    "- Spark will automatically apply the data structure to the data being impoted into the schema table. If there is an error, you ll see what that error is and if there i not error your data is imported accurately. This is the power of creating a sturctured schema in Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde76248-f452-4a50-bf8d-1cd841a7376b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Repeat above process for other 4 datasets which we have to use\n",
    "\n",
    "Data sets to be used : Match.csv, Player_match.csv, Player.csv and Team.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b8fd23-e71b-4159-a821-913ece505702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data ingestion for Match.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e039945-6925-497e-8dbe-e7ac27e67c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Match.csv extraction\n",
    "\n",
    "match_schema = StructType([\n",
    "    StructField(\"match_sk\", IntegerType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season_year\",IntegerType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmatch\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a59bbb2-2a4a-476a-9ecc-94a563996390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing match.csv data from S3 bucket\n",
    "\n",
    "match_df = spark.read.schema(match_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project-deproj/Match.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b9ce32c1-6389-481b-9acd-e1a04d3f76c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-\n",
    "#### --- Player.csv ---\n",
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "])\n",
    "\n",
    "#### --- Player_match.csv ---\n",
    "##### Note: adjust DecimalType precision/scale for playermatch_key if needed.\n",
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(38, 0), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),      # 'year' → use IntegerType\n",
    "    StructField(\"is_manofthematch\", BooleanType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", BooleanType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True),\n",
    "])\n",
    "\n",
    "#### --- Team.csv ---\n",
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c222e7e8-bd41-4717-b05e-3500736cef79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Data ingestion for Player_match.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7bdccf9-22aa-4276-8bad-50b762d1661d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(38, 0), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),      # 'year' → use IntegerType\n",
    "    StructField(\"is_manofthematch\", BooleanType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", BooleanType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189e8ede-8f96-45c5-969e-e310ac2e59af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_match_df = spark.read.schema(player_match_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project-deproj/Player_match.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60cc3d2-0d0b-44df-b17f-87a0dcac90f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Data ingestion for Player.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9fbad1-5386-42ea-a8cf-e105f4848de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e492f7-2112-4cc4-8a35-b419b1a6910f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_df = spark.read.schema(player_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project-deproj/Player.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e9d294-0f7e-4f39-9d09-50f60c2f3f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Data ingestion for Team.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8925b48-838f-4c8b-973d-c3548f5d8ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af98365f-4188-46b2-b805-e635e84d9feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "team_df = spark.read.schema(team_schema).format(\"csv\").option(\"header\", \"true\").load(\"s3://ipl-data-analysis-project-deproj/Team.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc109357-4ac9-4928-be2a-7419d013e74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let us check the schema tables updated\n",
    "\n",
    "Ball_By_Ball_df.show(5)\n",
    "match_df.show(5)\n",
    "player_match_df.show(5)\n",
    "player_df.show(5)\n",
    "team_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a500ec3-5f73-4484-8aaf-3025ce0deb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##DATA TRANSFORMATIONS\n",
    "\n",
    "Transforms are crucial from the business point of view as they help in filtering, combining and arranging data as per the business requirements. For this project we will not focus on the business aspect but more on the technicalities involved to get the transform right.\n",
    "\n",
    "Before we begin the transformations, we need to call (import) some functions. We can do this by calling 'pyspark.sql.functions import *', this function imports all the functions from pyspark sql library.\n",
    "\n",
    "We can also specify the functions that we need for the task, like: 'pyspark.sql.functions import col, when, sum, avg, row_number'.\n",
    "\n",
    "Once the functions are imported we will then filter the ball by ball data and remove wides and no balls columns since they are not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfd6a97-8474-4fe6-8dea-4a80b10e09c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Ball by ball data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa6a336-8998-4136-a017-c06b35ae9254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum, avg, row_number\n",
    "\n",
    "#Filter ball by ball data and remove wides and noballs\n",
    "Ball_By_Ball_df = Ball_By_Ball_df.filter((col(\"wides\") == 0) & (col(\"noballs\") == 0))\n",
    "\n",
    "# In Apache spark, the transformations are lazy, so we need to call the action to execute the transformations. We have few more transforms to apply so we will call the action at the end.\n",
    "\n",
    "# Let us create a new aggregated df with total and avg runs scored in each match and inning.\n",
    "total_and_avg_runs = Ball_By_Ball_df.groupBy(\"match_id\", \"innings_no\").agg(\n",
    "    sum(\"runs_scored\").alias(\"total_runs\"), \n",
    "    avg(\"runs_scored\").alias(\"average_runs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1aabafb-f1a1-4914-b159-7b32e954ef91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Window Functions: Calculate running totals of runs in each match for each over\n",
    "#For this we need to import window function. Follow: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Window.html\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#Create a window structure with partition by match_id and innings_no and order by over_id\n",
    "windowSpec = Window.partitionBy(\"match_id\", \"innings_no\").orderBy(\"over_id\")\n",
    "\n",
    "#We will now create a running total transform using the window structure created above\n",
    "\n",
    "Ball_By_Ball_df = Ball_By_Ball_df.withColumn(\n",
    "    \"running_total_runs\", \n",
    "    sum(\"runs_scored\").over(windowSpec)\n",
    ")\n",
    "\n",
    "#This will add a new column named \"running_total_runs\" at the end of ball by ball df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69897d05-ea6a-4e43-80b6-373dc55330b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Conditional Formatting - Flag wickets or 6 runs highlighting the high impact balls\n",
    "#We can also apply conditional formatting to flag different outcomes\n",
    "\n",
    "#Conditional columns: High impact balls\n",
    "\n",
    "Ball_By_Ball_df = Ball_By_Ball_df.withColumn(\n",
    "    \"high_impact\",\n",
    "    when((col(\"runs_scored\") + col(\"extra_runs\") > 6) | (col(\"bowler_wicket\") == True), True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8481e26a-f011-4ba5-b8aa-05abb52b9e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Action: to complete the alzy transforms we will run the show function to get the transformed columns\n",
    "\n",
    "Ball_By_Ball_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7a0076-04f5-4cce-99a5-9ae3dbd52788",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can follow similar steps to transform other data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21199d96-fecb-4c9c-87a7-de31bffe45eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Match data transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f611d392-fbd6-4662-9f76-e68dd46b69da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, when, col\n",
    "\n",
    "# Extract year, month, and day from match_date\n",
    "match_df = (\n",
    "    match_df\n",
    "        .withColumn(\"year\",  year(col(\"match_date\")))\n",
    "        .withColumn(\"month\", month(col(\"match_date\")))\n",
    "        .withColumn(\"day\",   dayofmonth(col(\"match_date\")))\n",
    ")\n",
    "\n",
    "# Categorize win margins into 'High', 'Medium', and 'Low'\n",
    "match_df = match_df.withColumn(\n",
    "    \"win_margin_category\",\n",
    "    when(col(\"win_margin\") >= 100, \"High\")\n",
    "    .when((col(\"win_margin\") >= 50) & (col(\"win_margin\") < 100), \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "# Analyze toss impact: who wins the toss and the match\n",
    "match_df = match_df.withColumn(\n",
    "    \"toss_match_winner\",\n",
    "    when(col(\"toss_winner\") == col(\"match_winner\"), \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "# Show the enhanced DataFrame\n",
    "match_df.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820dfc41-e1f6-431b-9f88-e29164056426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Player data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a0534f-ea37-409a-8ccc-1d0aedf4c274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, when, col\n",
    "\n",
    "# Normalize and clean player names\n",
    "player_df = player_df.withColumn(\n",
    "    \"player_name\",\n",
    "    lower(regexp_replace(col(\"player_name\"), r\"[^a-zA-Z0-9 ]\", \"\"))\n",
    ")\n",
    "\n",
    "# Handle missing values in 'batting_hand' and 'bowling_skill' with a default 'unknown'\n",
    "player_df = player_df.na.fill({\"batting_hand\": \"unknown\", \"bowling_skill\": \"unknown\"})\n",
    "\n",
    "# Categorize players based on batting hand\n",
    "player_df = player_df.withColumn(\n",
    "    \"batting_style\",\n",
    "    when(lower(col(\"batting_hand\")).contains(\"left\"), \"Left-Handed\").otherwise(\"Right-Handed\")\n",
    ")\n",
    "\n",
    "# Show the modified player DataFrame\n",
    "player_df.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ddb16f9-7950-4e54-9491-8bbe0485fade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Player Match data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53d6b9b-edb6-418b-a78a-ca01ceb0da80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, current_date, year\n",
    "\n",
    "# Add a 'veteran_status' column based on player age\n",
    "player_match_df = player_match_df.withColumn(\n",
    "    \"veteran_status\",\n",
    "    when(col(\"age_as_on_match\") >= 35, \"Veteran\").otherwise(\"Non-Veteran\")\n",
    ")\n",
    "\n",
    "# Dynamic column to calculate years since debut\n",
    "player_match_df = player_match_df.withColumn(\n",
    "    \"years_since_debut\",\n",
    "    year(current_date()) - col(\"season_year\")\n",
    ")\n",
    "\n",
    "# Show the enriched DataFrame\n",
    "player_match_df.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cbf7435-8aa8-4786-9e13-49c54eb73e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##SQL ANALYSIS: Gaining Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade382b2-1d42-4132-ae31-b52f3a89b2f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- From the updated data transform tables we will now create views for SQL analysis. This helps in less memory use and faster querying. \n",
    "- We will use the command : createOrReplaceGlobalTempView for all the dataframes we have.\n",
    "> - Note: For Serverless compute in Databricks, 'createOrReplaceGlobalTempView' is not supported. So we will use 'createOrReplaceTempView' function to create a temporary view for SQL analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e2b3b9-ef2a-45f1-95bd-422ef5dd4fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Ball_By_Ball_df.createOrReplaceTempView(\"ball_by_ball\")\n",
    "match_df.createOrReplaceTempView(\"match\")\n",
    "player_match_df.createOrReplaceTempView(\"player_match\")\n",
    "player_df.createOrReplaceTempView(\"player\")\n",
    "team_df.createOrReplaceTempView(\"team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243f5fb6-bf86-4d9d-abb4-3f0db3ef36f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's begin with our SQL analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a90247-e5a2-4116-9564-9d41f5408e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SQL View for Top Scoring Batsman per season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dec1c1b-3002-4b06-83dc-7c519316ae93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_scoring_batsman_per_season = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        p.player_name,\n",
    "        m.season_year,\n",
    "        SUM(b.runs_scored) AS total_runs\n",
    "    FROM ball_by_ball b\n",
    "    JOIN match m ON b.match_id = m.match_id\n",
    "    JOIN player_match pm ON m.match_id = pm.match_id AND b.striker = pm.player_id\n",
    "    JOIN player p ON p.player_id = pm.player_id\n",
    "    GROUP BY p.player_name, m.season_year\n",
    "    ORDER BY m.season_year, total_runs DESC\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d803ab4-1851-4364-9b2d-223185b01f5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_scoring_batsman_per_season.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f1dc60-9668-4c38-ae5c-44dff6804d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL View for Economical Bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd686b3b-dd0d-4392-a250-17bea2d26f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "economical_bowlers_powerplay = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        p.player_name, \n",
    "        ROUND(AVG(b.runs_scored),3) AS average_runs_per_ball,\n",
    "        COUNT(b.bowler_wicket) AS total_wickets\n",
    "        FROM ball_by_ball b\n",
    "        JOIN player_match pm ON b.match_id = pm.match_id AND b.bowler = pm.player_id\n",
    "        JOIN player p ON pm.player_id = p.player_id\n",
    "        WHERE b.over_id <= 6\n",
    "        GROUP BY p.player_name\n",
    "        HAVING COUNT(*) > 120\n",
    "        ORDER BY average_runs_per_ball, total_wickets DESC \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "economical_bowlers_powerplay.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5853ddb5-d5b6-4675-bfac-7daf68bbd17e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are 2 more queries. Check them after for practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7bca40-1458-49c9-a3e1-8edee027867f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##DATA VISUALIZATION\n",
    "\n",
    "We can perform data visualization using python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e353d6-8693-490d-bfb9-48bf2bb5b7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Economical Bowlers Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70800a7b-9d6e-4248-8cd6-b891271c7e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for visualization\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Using bowlers spark dataframe for pandas\n",
    "economocal_bowlers_pd = economical_bowlers_powerplay.toPandas()\n",
    "\n",
    "#Visualizing using Matplotlib\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "#Limiting the records to top 10 for better clarity in the plot\n",
    "top_economical_bowlers = economocal_bowlers_pd.nsmallest(10, 'average_runs_per_ball')\n",
    "plt.bar(top_economical_bowlers['player_name'], top_economical_bowlers['average_runs_per_ball'], color = 'skyblue')\n",
    "plt.xlabel('Bowler name')\n",
    "plt.ylabel('Average Runs per Ball')\n",
    "plt.title('Most Economical Bowlers in Powerplay Overs (Top 10)')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IPL_Data_Project-RAW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
